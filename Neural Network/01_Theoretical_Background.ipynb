{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "91b636ba15d245c5be493b1273662101"
   },
   "source": [
    "# Table of Contents\n",
    ">## 1. NN - Construction\n",
    "* 1.1. Logistic Function (Activation Function)\n",
    "* 1.2. Non-linear Basis Function\n",
    "* 1.3. Universal Approximation Theorem\n",
    "* 1.4. NAND Gate (a.k.a. Universal Gate)\n",
    "* 1.5. MLP\n",
    "\n",
    ">## 2. NN - Optimization\n",
    "* 2.1. Algorithm\n",
    "* 2.2. Objective Function\n",
    "* 2.3. Error Backpropagation\n",
    "* 2.4. Error Backpropagation- Proof\n",
    "* 2.5. Error Backpropagation - Example\n",
    "* 2.6. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NN - Construction\n",
    "* NN: Adaptive Basis Function Model\n",
    "* NN: MLP(Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "a70b52d45f214b8cb440ad6b6442d14f"
   },
   "source": [
    "## 1.1. Logistic Function (Activation Function)\n",
    "* **Form**:\n",
    "\n",
    ">$$ \n",
    "\\begin{eqnarray} \n",
    "z = h(a) = \\sigma(a) = \\frac{1}{1+e^{-a}} \n",
    "\\end{eqnarray} \n",
    "$$\n",
    ">\n",
    ">$$ \\hat{y} = \\text{sign}\\left(z - \\dfrac{1}{2}\\right) = \\text{round}(z) $$\n",
    "\n",
    "* **Derivative**:\n",
    "\n",
    ">$$ \\dfrac{d\\sigma(a)}{da} = \\sigma(a)(1-\\sigma(a)) = \\sigma(a)\\sigma(-a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "314ab9af6090415b87afb86a92837fc0"
   },
   "source": [
    "## 1.2. Non-linear Basis Function\n",
    "* **1. Original Form - cannot deal with non-linear problems like XOR**\n",
    "\n",
    ">$$ z = h(w^Tx+b)$$\n",
    "\n",
    "* **2. Replace $x$ with a set of $J$ basis functions**\n",
    "\n",
    ">$$ z = h \\left( \\sum_{j=1}^J w_j \\phi_j(x) + b \\right) = h \\left( w^T \\phi(x) + b \\right) $$\n",
    "\n",
    "* **3. Replace $x$ with a basis function controlled by a hyperparameter, $\\theta$**\n",
    "\n",
    ">$$ z = h \\left( w^T \\phi(x ; \\theta) + b \\right) $$\n",
    "\n",
    "* **4. MLP: Uses the logistic sigmoid fn. as its basis function**\n",
    "\n",
    ">$$ \n",
    "\\phi_j(x ; \\theta_j) = \\phi_j(x ; w^{(1)}_j, {b}^{(1)}_j) \n",
    "= h \\left(w_{j}^{(1)} x + b_j^{(1)} \\right)  \n",
    "$$\n",
    "\n",
    "* **5. Final Output**\n",
    "\n",
    ">$$ z = h \\left( \\sum_{j=1}^M w_j h \\left(w_{j}^{(1)} x + b_j^{(1)} \\right)  + b \\right) $$\n",
    ">\n",
    ">* $w^{(1)}, b^{(1)}$: Controlls the form of basis function\n",
    ">* $w, b$: Work as discriminant function & Decide the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b556c87932474592862e96cb5cbf1585"
   },
   "source": [
    "## 1.3. Universal Approximation Theorem\n",
    "\n",
    ">$$\\text{A feed-forward network with a single hidden layer}$$\n",
    ">\n",
    ">$$\\text{containing a finite number of neurons}$$\n",
    ">\n",
    ">$$\\text{can approximate continuous functions on compact subsets of } R^n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "1e726a58d1174945aa4cac8233451fd1"
   },
   "source": [
    "## 1.4. NAND Gate (a.k.a. Universal Gate)\n",
    "* **NAND**: False only if all inputs are True\n",
    "* **Any boolean function can be implemented by using a combination of NAND gates**\n",
    "* **Perceptron with $w_1=-2, w_2=-2, b=3$ forms a NAND gate**\n",
    "\n",
    "|x1|x2|AND|NAND|XOR|\n",
    "|-|-|-|-|-|\n",
    "|0|0|0|1|0|\n",
    "|0|1|0|1|1|\n",
    "|1|0|0|1|1|\n",
    "|1|1|1|0|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "a3a964496b58401a9e93648cfee38a72"
   },
   "source": [
    "## 1.5. MLP\n",
    "* Each Perceptron: **Neuron** or **Node**\n",
    "* Each Layer: Works as an **Adaptive Basis Function**\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/4dcef7b75de64023900c7f7edb7cbb2f.png\">\n",
    "\n",
    "* Output layer with multiple neurons $\\rightarrow$ Return **class-conditional probability**\n",
    "* Example) **MNIST data**: $28 \\times 28$ Neurons in Input Layer & $10$ Neurons in Output Layer\n",
    "\n",
    "<img src=\"https://datascienceschool.net/upfiles/90f2752671424cef846839b89ddcf6aa.png\">\n",
    "\n",
    "* **Vector Notation**:\n",
    "\n",
    ">$$ z^{(l)}_j = h \\left( a^{(l)}_j \\right) =  h \\left( \\sum_{i=1}^I w^{(l)}_{j,i} z^{(l-1)}_i + b^{(l)}_j \\right) $$\n",
    ">\n",
    ">* $w^{(l)}_{j,i}$: To $j$th Neuron in $l$th Layer / From $i$th Neuron in $l-1$th Layer\n",
    "\n",
    "* **Matrix Notation**:\n",
    "\n",
    ">$$ a^{(l)} = {W^{(l)}} z^{(l-1)} + b^{(l)} $$\n",
    ">\n",
    ">$$ z^{(l)} = h(a^{(l)}) $$\n",
    ">\n",
    ">$$ z^{(l)} = h\\left({W^{(l)}} z^{(l-1)} + b^{(l)}\\right) $$\n",
    ">\n",
    "\n",
    "* **Input & Output**:\n",
    "\n",
    ">$$ \\text{Input: } z^{(0)} = x $$\n",
    ">\n",
    ">$$ \\text{Output: } \\hat{y} = z^{(L)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NN - Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c6202c0bea984c0ebbe72c4e57c94bb2"
   },
   "source": [
    "## 2.1. Algorithm\n",
    "\n",
    "#### Step 1. Initialization ($w$ and $b$)\n",
    "#### Step 2. Input\n",
    "#### Step 3. Feedforward Propagation ($a$ and $z$)\n",
    "#### Step 4. Output and Error Calculation ($z^{(L)}$ and $\\delta^{(L)}$)\n",
    "#### Step 5. Error Backpropagation ($\\delta$)\n",
    "#### Step 6. Gradient Calculation ($\\frac{\\partial C_i}{\\partial w}=z\\delta$, $\\frac{\\partial C_i}{\\partial b}=\\delta$ for $x_i$)\n",
    "#### Step 7. Minibatch-size Iteration\n",
    "#### Step 8. Weight Update ($w$ and $b$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "82ae7f1c088f4c4e9c54904798ca85f8"
   },
   "source": [
    "## 2.2. Objective Function\n",
    "* Output: conditional probability (set of real values) $\\therefore$ We use SS\n",
    "\n",
    ">$$ C(w,b) = \\sum_{i=1}^N C_i(w,b) =  \\sum_{i=1}^N \\| y_i - z_i^{(L)}(w,b) \\|^2 $$\n",
    ">\n",
    ">* $y$: true values / one-hot-encoding vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f734d836d73e4cc3912eb215f6f9c1cc"
   },
   "source": [
    "## 2.3. Error Backpropagation\n",
    "\n",
    "#### Step 1. Steepest Gradient Descent (standard form)\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  w_{k+1}  &=& w_{k} - \\mu \\frac{\\partial C}{\\partial w} \\\\\n",
    "  b_{k+1} &=& b_{k} - \\mu \\frac{\\partial C}{\\partial b}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    ">\n",
    "> $\\mu$: step size\n",
    "\n",
    "#### Step 2. Define $\\delta$\n",
    "\n",
    ">$$\n",
    "\\delta_j^{(l)} = \\dfrac{\\partial C}{\\partial a_j^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\delta^{(l-1)}_j = h'(a^{(l-1)}_j) \\sum_{i=1}^{N_{(l)}} w^{(l)}_{ij} \\delta^{(l)}_i\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "#### Step 3. $\\odot$: Hadamard Product (or Schur Product or Element-wise Product)\n",
    "\n",
    ">$$\n",
    "x \\odot y = \n",
    "\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right) \\odot\n",
    "\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\end{array}\\right) \n",
    "= \\left(\\begin{array}{c} x_1 y_1 \\\\ x_2 y_2 \\\\ x_3 y_3 \\end{array}\\right)\n",
    "$$\n",
    "\n",
    "#### Step 4. Rewrite (2) in vector-matrix notation\n",
    "\n",
    ">$$\n",
    "\\delta^{(l-1)} = h'(a^{(l-1)}) \\odot ({W^T}^{(l)} \\delta^{(l)}) \n",
    "$$\n",
    "\n",
    "#### Step 5. $\\delta^{(L)}_j$ (Output Layer)\n",
    "\n",
    ">$$ C = \\dfrac{1}{2}(y - z)^2 $$\n",
    ">\n",
    ">$$\n",
    "\\delta^{(L)}_j = y_j - z_j\n",
    "$$\n",
    "\n",
    "#### Step 6. Derivative w.r.t. Weights\n",
    "\n",
    ">$$\n",
    "\\frac{\\partial C}{\\partial w^{(l)}_{ji}} = \\delta^{(l)}_j z^{(l-1)}_i \n",
    "$$\n",
    "\n",
    "#### Step 7. Derivative w.r.t. Bias\n",
    "\n",
    ">$$\n",
    "\\frac{\\partial C}{\\partial b^{(l)}_{j}} = \\delta^{(l)}_j \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Error Backpropagation - Proof\n",
    "#### Step 1. Apply Chain Rule\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\delta^{(l)}_j & = & \\frac{\\partial C}{\\partial a^{(l)}_j} \\\\\n",
    "  & = & \\sum_i \\frac{\\partial C}{\\partial a^{(l+1)}_i} \\frac{\\partial a^{(l+1)}_i}{\\partial a^{(l)}_j} \\\\ \n",
    "  & = & \\sum_i \\delta^{(l+1)}_i \\frac{\\partial a^{(l+1)}_i}{\\partial a^{(l)}_j} \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "#### Step 2.\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  a^{(l+1)}_i = \\sum_j w^{(l+1)}_{ij} z^{(l)}_j + b^{(l+1)}_i = \\sum_j w^{(l+1)}_{ij} h (a^{(l)}_j) + b^{(l+1)}_i\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "#### Step 3.\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial a^{(l+1)}_i}{\\partial a^{(l)}_j} = w^{(l+1)}_{ij} h '(a^{(l)}_j)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "#### Step 4.\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\delta^{(l)}_j = \\sum_i \\delta^{(l+1)}_i w^{(l+1)}_{ij} h '(a^{(l)}_j) = h '(a^{(l)}_j) \\sum_i \\delta^{(l+1)}_i w^{(l+1)}_{ij} \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "#### Step 5. Proof for (Step 6 of 1.8.)\n",
    "\n",
    ">$$\n",
    "\\frac{\\partial C}{\\partial w^{(l)}_{ji}} = \\frac{\\partial C}{\\partial a^{(l)}_{j}}  \\frac{\\partial a^{(l)}_{j}}{\\partial w^{(l)}_{ji}} \n",
    "= \\delta^{(l)}_j z^{(l-1)}_i \n",
    "$$\n",
    "\n",
    "#### Step 6. Proof for (Step 7 of 1.8.)\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}  \n",
    "\\frac{\\partial C}{\\partial b^{(l)}_j} =   \\delta^{(l)}_j\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Error Backpropagation - Example\n",
    "#### Output Layer (2 Nodes)\n",
    "\n",
    ">$$ \\delta^{(3)} = y - z^{(3)} $$\n",
    "\n",
    "#### 2nd Hidden Layer (3 Nodes)\n",
    "\n",
    ">$$ \\frac{\\partial C}{\\partial w^{(3)}_{jk}} = z^{(2)}_k \\delta^{(3)}_j $$\n",
    ">\n",
    ">$$ \\delta^{(2)} = h'(a^{(2)}) \\odot ((W^{(3)})^T \\delta^{(3)}) $$\n",
    "\n",
    "#### 1st Hidden Layer (3 Nodes)\n",
    "\n",
    ">$$ \\frac{\\partial C}{\\partial w^{(2)}_{jk}} = z^{(1)}_k \\delta^{(2)}_j $$\n",
    ">\n",
    ">$$ \\delta^{(1)} = h'(a^{(1)}) \\odot ((W^{(2)})^T \\delta^{(2)}) $$\n",
    "\n",
    "#### Input Layer (2 Nodes)\n",
    "\n",
    ">$$ \\frac{\\partial C}{\\partial w^{(1)}_{jk}} = x_k \\delta^{(1)}_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c728de1f3f634ecbb165a3410e0c72aa"
   },
   "source": [
    "##  2.6. Stochastic Gradient Descent\n",
    "\n",
    "#### Total Cost = Sum of $N$ Costs\n",
    "\n",
    ">$$ C = \\sum_{i=1}^N C_i $$\n",
    ">\n",
    ">$$ \\dfrac{\\partial C}{\\partial w_k} = \\sum_{i=1}^N \\dfrac{\\partial C_i}{\\partial w_k} $$\n",
    "\n",
    "#### Final Update\n",
    "\n",
    ">$$ w_{k+1} = w_k - \\mu \\dfrac{\\partial C}{\\partial w_k}  $$\n",
    "\n",
    "#### Such process is time-consuming $\\rightarrow$ use $M$ samples to approximate gradient\n",
    "\n",
    ">$$ \\tilde{\\dfrac{\\partial C}{\\partial w_k}} = \\dfrac{N}{M} \\sum_{i=1}^M \\dfrac{\\partial C_i}{\\partial w_k} \\;\\;\\;\\; (M < N) $$\n",
    ">\n",
    ">$$ w_{k+1} = w_k - \\mu \\tilde{\\dfrac{\\partial C}{\\partial w_k}}  $$\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    ">1. From $N$ samples, use $M$ to update weights & bias ($M$: mini-batch size)\n",
    ">2. Continue by selecting $M$ from un-used samples \n",
    ">3. Repeat for $\\dfrac{N}{M}$ times (1 epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
